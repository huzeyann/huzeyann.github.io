---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Motto. Etc.

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>üìçPenn </p></br>
    <p><a href="mailto:%68%75%7A%65.%79%61%6E%6E@%67%6D%61%69%6C.%63%6F%6D">Email</a></p></br>
    <p><a href="https://github.com/huzeyann" target="_blank" title="GitHub">Github</a></p></br>
    <p><a href="https://twitter.com/HuzeYann" target="_blank" title="Twitter">Twitter</a></p></br>

news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

PhD student at University of Pennsylvania, co-advised by [James C. Gee](https://www.med.upenn.edu/apps/faculty/index.php/g5455356/p10656){:target="\_blank"} and [Jianbo Shi](https://www.cis.upenn.edu/~jshi/){:target="\_blank"}. 

<!-- I have been nerding out on <b>Brain Encoding Models</b> since we met. I work on computer vision and computational neuroscience. -->

1. Computer Vision: segmentation, correspondence, un-supervised methods

2. Explainbility: feature visualization, mechanistic interpretability

Before joining academia, I was at CNSS DevOps team. We had great fun in Hackathons

1. Software Development: Web App, Embedded OS, Unity

2. Operations: Deployment, Containers, Networking systems

<br/>
<br/>

I play rhythm game and instruments when things doesn't work <small>(they doesn't work most of the time)</small>. My favorite: Blues Driver & Telecaster

<br/><br/><br/><br/><br/><br/><br/> 

<hr>

<h2>Softwares </h2>

<font face="helvetica, ariel, 'sans serif'">
            <table cellspacing="15">
				<tbody>
                <tr>
                    <td width="40%" align="center">
                    <a href="https://ncut-pytorch.readthedocs.io/">
                        <video width="90%" controls="" muted="" autoplay="" loop="">
                        <source src="/assets/videos/ncut_video_sam_264_small.mp4" type="video/mp4">
                        </video>
                    </a> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>ncut-pytorch</b>: Nystr√∂m Normalized Cuts PyTorch <br><br>
                        <span style="font-size: 10pt;">
                        [<a href="https://huggingface.co/spaces/huzey/ncut-pytorch">Huggingface Demo</a>] <br>
                        [<a href="https://ncut-pytorch.readthedocs.io/">Python Package</a>] <br>
                        [<a href="https://ncut-pytorch.readthedocs.io/">Website</a>] <br>
                        [<a href="https://penno365-my.sharepoint.com/:p:/g/personal/huze_upenn_edu/ESmESKMkHONGm-UMYz1X2A4BmjlT4Qmlv63QaJq8Z1ButQ?e=gvr5e1">Slides</a>] <br>
                        [Paper (coming)]
                        <br>
                    </span></span></td>
                </tr>
                </tbody>
            </table>
            
</font>


<br/>
<hr>

<h2>Publications </h2>

<font face="helvetica, ariel, 'sans serif'">
            <table cellspacing="15">
				<tbody>
                <tr>
                    <td width="30%" align="center">
                        <img width="225" align="middle" src="assets/gif/m1c.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>"I Know It When I See It": Mood Spaces for Connecting and Expressing Visual Concepts</b> <br>
                        <span style="font-size: 10pt;">
                        Huzheng Yang, Katherine Xu, Michael D. Grossberg, Yutong Bai, Jianbo Shi<br>
                        Mar. 2025 <br>
                        arxiv <br>
                        [<a href="https://arxiv.org/abs/2504.15145">Paper</a>]
                        [<a href="https://huzeyann.github.io/mspace/">Webpage</a>]
                        [<a href="https://github.com/huzeyann/mood">GitHub</a>]
                        [<a href="https://huggingface.co/spaces/huzey/MoodSpace">Demo</a>]
                        <br>
                    </span></span></td>
                </tr>
                <tr>
                    <td width="30%" align="center">
                        <img width="225" align="middle" src="/assets/img/alignedcut.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>AlignedCut: Visual Concepts Discovery on Brain-Guided Universal Feature Space</b> <br>
                        <span style="font-size: 10pt;">
                        Huzheng Yang, James Gee*, Jianbo Shi*<br>
                        May. 2024 <br>
                        arxiv, <em>rejected by NeurIPS</em> <br>
                        [<a href="https://arxiv.org/abs/2406.18344">Paper</a>]
                        [<a href="https://penno365-my.sharepoint.com/:p:/g/personal/huze_upenn_edu/EdPlbbmV3Q9DmXKRIElcw5ABhOnNQctJSky6jcN7Au0elg?e=snyVLo">Slides</a>]
                        <br>
                    </span></span></td>
                </tr>
                <tr>
                    <td width="30%" align="center">
                        <img width="225" align="middle" src="assets/gif/brain_background.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Brain Decodes Deep Nets</b> <br>
                        <span style="font-size: 10pt;">
                        Huzheng Yang, James Gee*, Jianbo Shi*<br>
                        Nov. 2023 <br>
                        CVPR'24, <b><em><a>highlight</a></em></b> <br>
                        [<a href="https://arxiv.org/abs/2312.01280">Paper</a>]
                        [<a href="https://huzeyann.github.io/brain-decodes-deep-nets">Webpage</a>]
                        [<a href="https://github.com/huzeyann/BrainDecodesDeepNets">GitHub</a>]
                        [<a href="https://penno365-my.sharepoint.com/:p:/g/personal/huze_upenn_edu/EVDLndCXy21LpKEelu_MVkMBK9dbFIhlI6VEQzOl4j6eLA?e=eED63x">Slides</a>]
                        [<a href="https://youtu.be/Qh49zQQCW1g">Talk online</a>]
                        <br>
                    </span></span></td>
                </tr>
                <tr>
                    <td width="30%" align="center">
                        <img width="225" align="middle" src="assets/custom_images/small_brain.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Memory Encoding Model</b> <br>
                        <span style="font-size: 10pt;">
                        Huzheng Yang, James Gee*, Jianbo Shi*<br>
                        Aug. 2023 <br>
                        arxiv, <b><em><a style="color:DarkOrange;" href="http://algonauts.csail.mit.edu/archive.html">Algonauts 2023 challenge winner</a></em></b> <br>
                        [<a href="https://arxiv.org/abs/2308.01175">Paper</a>]
                        [<a href="https://huzeyann.github.io/mem">Webpage</a>]
                        [<a href="https://github.com/huzeyann/MemoryEncodingModel">GitHub</a>]
                        [<a href="https://penno365-my.sharepoint.com/:p:/g/personal/huze_upenn_edu/EewQz_XbSpJCtm63dl7WSSkBOeRDEfzGY8rcrwsmm5KvgA?e=Tw9hAv">Slides</a>]
                        [<a href="https://www.youtube.com/live/9Xh55mcWJeE?si=aCdlPM1MnBaainIF&t=3343">Talk CCN'23</a>]
                        <br>
                    </span></span></td>
                </tr>
                <tr>
                    <td width="30%" align="center">
                        <img width="225" align="middle" src="assets/custom_images/small_rm.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Retinotopy Inspired Brain Encoding Model and the All-for-One Training Recipe</b> <br>
                        <span style="font-size: 10pt;">
                        Huzheng Yang, Jianbo Shi*, James Gee*<br>
                        May. 2023 <br>
                        arxiv, <em>rejected by NeurIPS</em> <br>
                        [<a href="https://arxiv.org/abs/2307.14021">Paper</a>]
                        [<a href="https://openreview.net/forum?id=DvRTU1whxF">Openreview</a>]
                        <br>
                    </span></span></td>
                </tr>
                <tr>
                    <td width="30%" align="center">
                        <img width="225" align="middle" src="assets/custom_images/me_fmri.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Upgrading Voxel-wise Encoding Model via Integrated Integration over Features and Brain Networks</b> <br>
                        <span style="font-size: 10pt;">
                        Yuanning Li*, Huzheng Yang*, Shi Gu<br>
                        Nov. 2022 <br>
                        Science Bulletin <br>
                        [<a href="https://www.sciencedirect.com/science/article/pii/S2095927324001373">Paper</a>]
                        [<a href="https://github.com/huzeyann/htROI-neural-encoding">GitHub</a>]
                        <br>
                    </span></span></td>
                </tr>
                <tr>
                    <td width="30%" align="center">
                        <img width="225" align="middle" src="assets/custom_images/forg.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Effective Ensemble of Deep Neural Networks Predicts Neural Responses to Naturalistic Videos</b> <br>
                        <span style="font-size: 10pt;">
                        Huzheng Yang, Shanghang Zhang, Yifan Wu, Yuanning Li*, Shi Gu*<br>
                        Aug. 2021 <br>
                        bioRxiv, <b><em><a style="color:DarkOrange;" href="http://algonauts.csail.mit.edu/2021/index.html">Algonauts 2021 challenge winner</a></em></b> <br>
                        [<a href="https://www.biorxiv.org/content/10.1101/2021.08.24.457581.abstract">Paper</a>]
                        [<a href="https://github.com/huzeyann/huze_algonauts21">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=xtSh_XotVlo">Talk CCN'21</a>]
                        <br>
                    </span></span></td>
                </tr>



            </tbody></table>
            
</font>
